"""
üîó correlation_service.py
Servicio de An√°lisis de Correlaci√≥n Avanzada Solar-Econ√≥mica
Autor: Benjamin Cabeza Dur√°n (mechmind-dwv)
Asistente: DeepSeek AI

Sistema unificado de an√°lisis de correlaci√≥n que integra:
- Correlaci√≥n estad√≠stica multivariada
- An√°lisis espectral cruzado (FFT, wavelets)
- Causalidad de Granger y transferencia de entrop√≠a
- Detecci√≥n de ciclos comunes solares-econ√≥micos
- An√°lisis de sincronizaci√≥n de fase
- Modelos de regresi√≥n avanzados
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
import warnings

# An√°lisis estad√≠stico avanzado
from scipy import stats
from scipy.signal import correlate, correlation_lags, coherence, csd
from scipy.fft import fft, fftfreq
from scipy.optimize import curve_fit
import statsmodels.api as sm
from statsmodels.tsa.stattools import grangercausalitytests, adfuller
from statsmodels.tsa.vector_ar.var_model import VAR

# Machine Learning para correlaci√≥n no lineal
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mutual_info_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# An√°lisis de series temporales
import pywt  # Wavelets

warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class CorrelationMethod(Enum):
    """M√©todos de an√°lisis de correlaci√≥n"""
    PEARSON = "pearson"
    SPEARMAN = "spearman"
    KENDALL = "kendall"
    MUTUAL_INFORMATION = "mutual_information"
    CROSS_CORRELATION = "cross_correlation"
    WAVELET_COHERENCE = "wavelet_coherence"
    GRANGER_CAUSALITY = "granger_causality"

class CorrelationStrength(Enum):
    """Fuerza de la correlaci√≥n"""
    VERY_STRONG = "Muy Fuerte"
    STRONG = "Fuerte"
    MODERATE = "Moderada"
    WEAK = "D√©bil"
    VERY_WEAK = "Muy D√©bil"
    NONE = "No Significativa"

@dataclass
class CorrelationResult:
    """Resultado completo de an√°lisis de correlaci√≥n"""
    economic_indicator: str
    solar_indicator: str
    timestamp: datetime
    methods: Dict[str, float]  # M√©todo -> valor correlaci√≥n
    optimal_lag: int
    lag_correlation: float
    p_value: float
    confidence_interval: Tuple[float, float]
    significance: CorrelationStrength
    sample_size: int
    stationarity_test: Dict[str, Any]
    notes: List[str]

@dataclass
class SpectralAnalysis:
    """An√°lisis espectral cruzado"""
    common_periods: List[float]
    coherence_spectrum: Dict[float, float]
    phase_synchronization: Dict[float, float]
    shared_cycles: List[Dict[str, Any]]
    dominant_frequencies: List[Dict[str, Any]]
    wavelet_coherence: Dict[str, Any]

@dataclass
class CausalAnalysis:
    """An√°lisis de causalidad"""
    cause: str
    effect: str
    granger_causality: Dict[str, float]  # lags -> p-values
    transfer_entropy: float
    convergent_cross_mapping: float
    confidence: float
    direction: str  # 'solar_to_economic', 'economic_to_solar', 'bidirectional', 'none'

@dataclass
class CycleSynchronization:
    """Sincronizaci√≥n de ciclos"""
    solar_cycle_period: float
    economic_cycle_period: float
    period_ratio: float
    phase_difference: float
    synchronization_strength: float
    coherence: float
    historical_evidence: List[Dict[str, Any]]

class CorrelationService:
    """
    Servicio avanzado de an√°lisis de correlaci√≥n solar-econ√≥mica
    Implementa m√©todos estad√≠sticos modernos para detectar relaciones complejas
    """
    
    def __init__(self):
        self.correlation_cache = {}
        self.spectral_cache = {}
        self.causal_models = {}
        self.scaler = StandardScaler()
        
        # Configuraci√≥n de an√°lisis
        self.correlation_thresholds = {
            'very_strong': 0.8,
            'strong': 0.6,
            'moderate': 0.4,
            'weak': 0.2,
            'very_weak': 0.1
        }
        
        self.max_lag_months = 60  # M√°ximo lag para an√°lisis (5 a√±os)
        self.min_sample_size = 24  # M√≠nimo de puntos de datos (2 a√±os mensual)
        
        # Ciclos conocidos para an√°lisis espectral
        self.known_cycles = {
            'solar_11_year': 11.0,
            'solar_22_year': 22.0,
            'solar_gleissberg': 87.0,
            'kondratiev': 54.0,
            'kuznets': 18.0,
            'juglar': 9.0,
            'kitchin': 4.0,
            'seasonal_1_year': 1.0
        }
        
        logger.info("üîó Inicializado Servicio de Correlaci√≥n Avanzada")
    
    async def analyze_correlation(self, economic_data: pd.Series, 
                                solar_data: pd.Series,
                                economic_indicator: str = "Unknown",
                                solar_indicator: str = "Unknown") -> CorrelationResult:
        """
        An√°lisis de correlaci√≥n completo entre series econ√≥micas y solares
        
        Args:
            economic_data: Serie temporal econ√≥mica
            solar_data: Serie temporal solar
            economic_indicator: Nombre del indicador econ√≥mico
            solar_indicator: Nombre del indicador solar
            
        Returns:
            Resultado detallado del an√°lisis de correlaci√≥n
        """
        logger.info(f"üìà Analizando correlaci√≥n {economic_indicator} - {solar_indicator}")
        
        try:
            # Validar y preparar datos
            economic_clean, solar_clean = self._prepare_correlation_data(economic_data, solar_data)
            
            if len(economic_clean) < self.min_sample_size:
                raise ValueError(f"Muestra insuficiente: {len(economic_clean)} puntos")
            
            # Realizar m√∫ltiples an√°lisis de correlaci√≥n
            correlation_methods = self._compute_all_correlations(economic_clean, solar_clean)
            
            # Encontrar lag √≥ptimo
            optimal_lag, lag_correlation = self._find_optimal_lag(economic_clean, solar_clean)
            
            # Tests de significancia y estacionariedad
            p_value = self._compute_significance(economic_clean, solar_clean)
            stationarity = self._test_stationarity(economic_clean, solar_clean)
            confidence_interval = self._compute_confidence_interval(correlation_methods['pearson'], len(economic_clean))
            
            # Determinar fuerza de la correlaci√≥n
            significance = self._determine_correlation_strength(correlation_methods['pearson'])
            
            # Generar notas interpretativas
            notes = self._generate_correlation_notes(correlation_methods, optimal_lag, significance)
            
            result = CorrelationResult(
                economic_indicator=economic_indicator,
                solar_indicator=solar_indicator,
                timestamp=datetime.now(),
                methods=correlation_methods,
                optimal_lag=optimal_lag,
                lag_correlation=lag_correlation,
                p_value=p_value,
                confidence_interval=confidence_interval,
                significance=significance,
                sample_size=len(economic_clean),
                stationarity_test=stationarity,
                notes=notes
            )
            
            # Cachear resultado
            cache_key = f"{economic_indicator}_{solar_indicator}"
            self.correlation_cache[cache_key] = result
            
            logger.info(f"‚úÖ Correlaci√≥n analizada: {significance.value} (r={correlation_methods['pearson']:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis de correlaci√≥n: {e}")
            raise
    
    def _prepare_correlation_data(self, economic_data: pd.Series, 
                                solar_data: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """Preparar y limpiar datos para an√°lisis de correlaci√≥n"""
        # Alinear √≠ndices temporales
        common_index = economic_data.index.intersection(solar_data.index)
        
        if len(common_index) == 0:
            raise ValueError("No hay fechas comunes entre las series")
        
        economic_aligned = economic_data.loc[common_index]
        solar_aligned = solar_data.loc[common_index]
        
        # Remover valores faltantes
        economic_clean = economic_aligned.dropna().values
        solar_clean = solar_aligned.dropna().values
        
        # Verificar que tengan la misma longitud despu√©s de la limpieza
        min_length = min(len(economic_clean), len(solar_clean))
        economic_clean = economic_clean[:min_length]
        solar_clean = solar_clean[:min_length]
        
        # Remover tendencias lineales
        economic_detrended = self._remove_trend(economic_clean)
        solar_detrended = self._remove_trend(solar_clean)
        
        return economic_detrended, solar_detrended
    
    def _remove_trend(self, series: np.ndarray) -> np.ndarray:
        """Remover tendencia lineal de una serie"""
        x = np.arange(len(series))
        slope, intercept = np.polyfit(x, series, 1)
        trend = slope * x + intercept
        return series - trend
    
    def _compute_all_correlations(self, economic_data: np.ndarray, 
                                solar_data: np.ndarray) -> Dict[str, float]:
        """Calcular m√∫ltiples medidas de correlaci√≥n"""
        methods = {}
        
        # Correlaci√≥n de Pearson (lineal)
        pearson_corr, pearson_p = stats.pearsonr(economic_data, solar_data)
        methods['pearson'] = pearson_corr
        
        # Correlaci√≥n de Spearman (monot√≥nica)
        spearman_corr, spearman_p = stats.spearmanr(economic_data, solar_data)
        methods['spearman'] = spearman_corr
        
        # Correlaci√≥n de Kendall (ordinal)
        kendall_corr, kendall_p = stats.kendalltau(economic_data, solar_data)
        methods['kendall'] = kendall_corr
        
        # Informaci√≥n Mutua (no lineal)
        mi_score = mutual_info_score(
            self._discretize_data(economic_data),
            self._discretize_data(solar_data)
        )
        methods['mutual_information'] = mi_score
        
        # Correlaci√≥n de distancia (no param√©trica)
        distance_corr = self._compute_distance_correlation(economic_data, solar_data)
        methods['distance_correlation'] = distance_corr
        
        return methods
    
    def _discretize_data(self, data: np.ndarray, bins: int = 10) -> np.ndarray:
        """Discretizar datos continuos para informaci√≥n mutua"""
        return np.digitize(data, np.histogram(data, bins=bins)[1])
    
    def _compute_distance_correlation(self, x: np.ndarray, y: np.ndarray) -> float:
        """Calcular correlaci√≥n de distancia"""
        # Implementaci√≥n simplificada de distance correlation
        n = len(x)
        
        # Matrices de distancia
        a = np.abs(x[:, np.newaxis] - x)
        b = np.abs(y[:, np.newaxis] - y)
        
        # Centrado de matrices
        a_centered = a - a.mean(axis=0) - a.mean(axis=1)[:, np.newaxis] + a.mean()
        b_centered = b - b.mean(axis=0) - b.mean(axis=1)[:, np.newaxis] + b.mean()
        
        # Producto escalar
        dcov = np.sqrt(np.sum(a_centered * b_centered) / (n ** 2))
        
        # Varianzas
        dvar_x = np.sqrt(np.sum(a_centered * a_centered) / (n ** 2))
        dvar_y = np.sqrt(np.sum(b_centered * b_centered) / (n ** 2))
        
        if dvar_x * dvar_y == 0:
            return 0.0
        
        return dcov / np.sqrt(dvar_x * dvar_y)
    
    def _find_optimal_lag(self, economic_data: np.ndarray, 
                         solar_data: np.ndarray) -> Tuple[int, float]:
        """Encontrar el lag √≥ptimo entre series"""
        # Normalizar series
        economic_norm = (economic_data - np.mean(economic_data)) / np.std(economic_data)
        solar_norm = (solar_data - np.mean(solar_data)) / np.std(solar_data)
        
        # Calcular correlaci√≥n cruzada
        cross_corr = correlate(economic_norm, solar_norm, mode='full')
        lags = correlation_lags(len(economic_norm), len(solar_norm), mode='full')
        
        # Limitar a lags razonables (5 a√±os m√°ximo)
        max_lag_idx = min(self.max_lag_months, len(lags) // 2)
        valid_indices = np.where(np.abs(lags) <= max_lag_idx)[0]
        
        cross_corr_valid = cross_corr[valid_indices]
        lags_valid = lags[valid_indices]
        
        # Encontrar lag con m√°xima correlaci√≥n absoluta
        max_idx = np.argmax(np.abs(cross_corr_valid))
        optimal_lag = lags_valid[max_idx]
        
        # Normalizar correlaci√≥n m√°xima
        max_correlation = cross_corr_valid[max_idx] / (len(economic_norm) * np.std(economic_norm) * np.std(solar_norm))
        
        return optimal_lag, max_correlation
    
    def _compute_significance(self, economic_data: np.ndarray, 
                            solar_data: np.ndarray) -> float:
        """Calcular significancia estad√≠stica usando permutaci√≥n"""
        n_permutations = 1000
        original_corr = stats.pearsonr(economic_data, solar_data)[0]
        
        # Test de permutaci√≥n
        permuted_corrs = []
        for _ in range(n_permutations):
            shuffled_solar = np.random.permutation(solar_data)
            perm_corr = stats.pearsonr(economic_data, shuffled_solar)[0]
            permuted_corrs.append(perm_corr)
        
        # Calcular p-value
        p_value = np.sum(np.abs(permuted_corrs) >= np.abs(original_corr)) / n_permutations
        return p_value
    
    def _test_stationarity(self, economic_data: np.ndarray, 
                          solar_data: np.ndarray) -> Dict[str, Any]:
        """Realizar tests de estacionariedad"""
        stationarity = {}
        
        # Test ADF para estacionariedad
        adf_economic = adfuller(economic_data)
        adf_solar = adfuller(solar_data)
        
        stationarity['economic_adf'] = {
            'test_statistic': adf_economic[0],
            'p_value': adf_economic[1],
            'is_stationary': adf_economic[1] < 0.05
        }
        
        stationarity['solar_adf'] = {
            'test_statistic': adf_solar[0],
            'p_value': adf_solar[1],
            'is_stationary': adf_solar[1] < 0.05
        }
        
        stationarity['both_stationary'] = (
            stationarity['economic_adf']['is_stationary'] and 
            stationarity['solar_adf']['is_stationary']
        )
        
        return stationarity
    
    def _compute_confidence_interval(self, correlation: float, 
                                   sample_size: int) -> Tuple[float, float]:
        """Calcular intervalo de confianza 95% para correlaci√≥n"""
        if sample_size <= 3:
            return (-1.0, 1.0)
        
        # Transformaci√≥n Z de Fisher
        z = np.arctanh(correlation)
        z_se = 1 / np.sqrt(sample_size - 3)
        
        # Intervalo de confianza 95%
        z_lower = z - 1.96 * z_se
        z_upper = z + 1.96 * z_se
        
        # Transformar de vuelta
        lower = np.tanh(z_lower)
        upper = np.tanh(z_upper)
        
        return (lower, upper)
    
    def _determine_correlation_strength(self, correlation: float) -> CorrelationStrength:
        """Determinar fuerza de la correlaci√≥n basada en umbrales"""
        abs_corr = abs(correlation)
        
        if abs_corr >= self.correlation_thresholds['very_strong']:
            return CorrelationStrength.VERY_STRONG
        elif abs_corr >= self.correlation_thresholds['strong']:
            return CorrelationStrength.STRONG
        elif abs_corr >= self.correlation_thresholds['moderate']:
            return CorrelationStrength.MODERATE
        elif abs_corr >= self.correlation_thresholds['weak']:
            return CorrelationStrength.WEAK
        elif abs_corr >= self.correlation_thresholds['very_weak']:
            return CorrelationStrength.VERY_WEAK
        else:
            return CorrelationStrength.NONE
    
    def _generate_correlation_notes(self, methods: Dict[str, float], 
                                  optimal_lag: int, 
                                  significance: CorrelationStrength) -> List[str]:
        """Generar notas interpretativas del an√°lisis"""
        notes = []
        
        # Nota sobre fuerza de correlaci√≥n
        notes.append(f"Correlaci√≥n {significance.value}")
        
        # Nota sobre linealidad vs no linealidad
        pearson = methods.get('pearson', 0)
        spearman = methods.get('spearman', 0)
        mi = methods.get('mutual_information', 0)
        
        if abs(pearson) < 0.3 and mi > 0.5:
            notes.append("Posible relaci√≥n no lineal detectada")
        elif abs(pearman) > abs(pearson) * 1.2:
            notes.append("Relaci√≥n monot√≥nica m√°s fuerte que lineal")
        
        # Nota sobre lag
        if abs(optimal_lag) > 12:
            notes.append(f"Lag significativo: {optimal_lag} meses")
        elif abs(optimal_lag) > 6:
            notes.append(f"Lag moderado: {optimal_lag} meses")
        elif optimal_lag != 0:
            notes.append(f"Lag menor: {optimal_lag} meses")
        else:
            notes.append("Sin lag significativo (correlaci√≥n contempor√°nea)")
        
        return notes
    
    async def cross_spectral_analysis(self, economic_data: pd.Series,
                                    solar_data: pd.Series) -> SpectralAnalysis:
        """
        An√°lisis espectral cruzado para detectar ciclos comunes
        
        Args:
            economic_data: Serie econ√≥mica
            solar_data: Serie solar
            
        Returns:
            An√°lisis espectral completo
        """
        logger.info("üìä Realizando an√°lisis espectral cruzado...")
        
        try:
            # Preparar datos
            economic_clean, solar_clean = self._prepare_correlation_data(economic_data, solar_data)
            
            # An√°lisis de Fourier
            common_periods = self._find_common_periods_fft(economic_clean, solar_clean)
            coherence_spectrum = self._compute_coherence(economic_clean, solar_clean)
            phase_sync = self._compute_phase_synchronization(economic_clean, solar_clean)
            
            # An√°lisis wavelet
            wavelet_coherence = self._compute_wavelet_coherence(economic_clean, solar_clean)
            
            # Identificar ciclos compartidos
            shared_cycles = self._identify_shared_cycles(common_periods, coherence_spectrum)
            dominant_frequencies = self._find_dominant_frequencies(economic_clean, solar_clean)
            
            analysis = SpectralAnalysis(
                common_periods=common_periods,
                coherence_spectrum=coherence_spectrum,
                phase_synchronization=phase_sync,
                shared_cycles=shared_cycles,
                dominant_frequencies=dominant_frequencies,
                wavelet_coherence=wavelet_coherence
            )
            
            self.spectral_cache['latest'] = analysis
            logger.info(f"‚úÖ Encontrados {len(common_periods)} per√≠odos comunes")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis espectral: {e}")
            raise
    
    def _find_common_periods_fft(self, economic_data: np.ndarray, 
                               solar_data: np.ndarray) -> List[float]:
        """Encontrar per√≠odos comunes usando FFT"""
        # Calcular FFT para ambas series
        fft_economic = np.abs(fft(economic_data))
        fft_solar = np.abs(fft(solar_data))
        
        # Frecuencias (asumiendo datos mensuales)
        n = len(economic_data)
        freqs = fftfreq(n, d=1/12)  # Frecuencia en ciclos por a√±o
        positive_freqs = freqs[:n//2]
        
        # Encontrar picos espectrales significativos
        economic_peaks = self._find_spectral_peaks(fft_economic[:n//2], positive_freqs)
        solar_peaks = self._find_spectral_peaks(fft_solar[:n//2], positive_freqs)
        
        # Encontrar per√≠odos comunes (dentro de 10% de tolerancia)
        common_periods = []
        for econ_period, econ_power in economic_peaks.items():
            for solar_period, solar_power in solar_peaks.items():
                if abs(econ_period - solar_period) / econ_period < 0.1:
                    common_periods.append((econ_period + solar_period) / 2)
        
        return sorted(common_periods)
    
    def _find_spectral_peaks(self, spectrum: np.ndarray, 
                           freqs: np.ndarray, 
                           min_prominence: float = 0.1) -> Dict[float, float]:
        """Encontrar picos significativos en el espectro"""
        from scipy.signal import find_peaks
        
        peaks, properties = find_peaks(spectrum, prominence=min_prominence * np.max(spectrum))
        
        significant_peaks = {}
        for peak_idx in peaks:
            if freqs[peak_idx] > 0:  # Ignorar frecuencia cero (tendencia)
                period = 1 / freqs[peak_idx]  # Per√≠odo en a√±os
                power = spectrum[peak_idx]
                significant_peaks[period] = power
        
        return significant_peaks
    
    def _compute_coherence(self, economic_data: np.ndarray, 
                         solar_data: np.ndarray) -> Dict[float, float]:
        """Calcular coherencia espectral"""
        f, Cxy = coherence(economic_data, solar_data, fs=12)  # fs=12 para datos mensuales
        
        coherence_spectrum = {}
        for freq, coh in zip(f, Cxy):
            if freq > 0:  # Ignorar frecuencia cero
                period = 1 / freq
                coherence_spectrum[period] = coh
        
        return coherence_spectrum
    
    def _compute_phase_synchronization(self, economic_data: np.ndarray, 
                                     solar_data: np.ndarray) -> Dict[float, float]:
        """Calcular sincronizaci√≥n de fase"""
        # Usar wavelets para an√°lisis de fase
        wavelet = 'cmor1.5-1.0'
        scales = np.arange(1, 128)
        
        coeffs_economic, freqs_economic = pywt.cwt(economic_data, scales, wavelet)
        coeffs_solar, freqs_solar = pywt.cwt(solar_data, scales, wavelet)
        
        # Calcular diferencia de fase
        phase_diff = np.angle(coeffs_economic) - np.angle(coeffs_solar)
        phase_sync = 1 - np.sin(phase_diff / 2) ** 2
        
        # Promediar sobre tiempo para cada escala/frecuencia
        avg_phase_sync = np.mean(phase_sync, axis=1)
        
        phase_synchronization = {}
        for scale, sync in zip(scales, avg_phase_sync):
            period = scale / 12  # Aproximaci√≥n de per√≠odo en a√±os
            phase_synchronization[period] = sync
        
        return phase_synchronization
    
    def _compute_wavelet_coherence(self, economic_data: np.ndarray, 
                                 solar_data: np.ndarray) -> Dict[str, Any]:
        """Calcular coherencia wavelet"""
        try:
            # Implementaci√≥n simplificada de coherencia wavelet
            wavelet = 'cmor1.5-1.0'
            scales = np.arange(1, 64)
            
            # Coeficientes wavelet
            coeffs_economic, freqs_economic = pywt.cwt(economic_data, scales, wavelet)
            coeffs_solar, freqs_solar = pywt.cwt(solar_data, scales, wavelet)
            
            # Coherencia wavelet (simplificada)
            cross_spectrum = coeffs_economic * np.conj(coeffs_solar)
            wavelet_coherence = np.abs(cross_spectrum) / (
                np.sqrt(np.abs(coeffs_economic)**2 * np.abs(coeffs_solar)**2)
            )
            
            # Encontrar regiones de alta coherencia
            high_coherence_regions = np.where(wavelet_coherence > 0.7)
            
            return {
                'coherence_matrix': wavelet_coherence,
                'scales': scales,
                'high_coherence_regions': high_coherence_regions,
                'max_coherence': np.max(wavelet_coherence)
            }
            
        except Exception as e:
            logger.warning(f"Error en coherencia wavelet: {e}")
            return {
                'coherence_matrix': np.array([]),
                'scales': np.array([]),
                'high_coherence_regions': (np.array([]), np.array([])),
                'max_coherence': 0.0
            }
    
    def _identify_shared_cycles(self, common_periods: List[float],
                              coherence_spectrum: Dict[float, float]) -> List[Dict[str, Any]]:
        """Identificar ciclos compartidos significativos"""
        shared_cycles = []
        
        for period in common_periods:
            # Encontrar coherencia m√°s cercana
            closest_period = min(coherence_spectrum.keys(), 
                               key=lambda x: abs(x - period))
            coherence_strength = coherence_spectrum.get(closest_period, 0)
            
            if coherence_strength > 0.5:  # Umbral de coherencia
                cycle_info = {
                    'period_years': period,
                    'coherence_strength': coherence_strength,
                    'cycle_type': self._classify_cycle_type(period),
                    'significance': 'Alta' if coherence_strength > 0.7 else 'Media',
                    'theoretical_match': self._find_theoretical_match(period)
                }
                shared_cycles.append(cycle_info)
        
        return sorted(shared_cycles, key=lambda x: x['coherence_strength'], reverse=True)
    
    def _find_dominant_frequencies(self, economic_data: np.ndarray, 
                                 solar_data: np.ndarray) -> List[Dict[str, Any]]:
        """Encontrar frecuencias dominantes en cada serie"""
        dominant_freqs = []
        
        for data, name in [(economic_data, 'economic'), (solar_data, 'solar')]:
            fft_data = np.abs(fft(data))
            freqs = fftfreq(len(data), d=1/12)
            
            # Encontrar picos significativos
            peaks = self._find_spectral_peaks(fft_data[:len(data)//2], freqs[:len(data)//2])
            
            for period, power in list(peaks.items())[:5]:  # Top 5 frecuencias
                dominant_freqs.append({
                    'series': name,
                    'period_years': period,
                    'power': power,
                    'cycle_type': self._classify_cycle_type(period)
                })
        
        return dominant_freqs
    
    def _classify_cycle_type(self, period: float) -> str:
        """Clasificar tipo de ciclo basado en per√≠odo"""
        # Buscar ciclo conocido m√°s cercano
        closest_cycle = min(self.known_cycles.values(), 
                          key=lambda x: abs(x - period))
        
        for name, known_period in self.known_cycles.items():
            if abs(known_period - period) / known_period < 0.2:  # 20% de tolerancia
                return name.replace('_', ' ').title()
        
        return f"Desconocido ({period:.1f} a√±os)"
    
    def _find_theoretical_match(self, period: float) -> Optional[str]:
        """Encontrar coincidencia con ciclos te√≥ricos"""
        for cycle_name, theoretical_period in self.known_cycles.items():
            if abs(theoretical_period - period) / theoretical_period < 0.15:  # 15% de tolerancia
                return cycle_name.replace('_', ' ').title()
        return None
    
    async def analyze_causality(self, economic_data: pd.Series,
                              solar_data: pd.Series,
                              max_lag: int = 12) -> CausalAnalysis:
        """
        Analizar causalidad entre series solares y econ√≥micas
        
        Args:
            economic_data: Serie econ√≥mica
            solar_data: Serie solar
            max_lag: M√°ximo n√∫mero de lags para test de Granger
            
        Returns:
            An√°lisis de causalidad completo
        """
        logger.info("üîç Analizando causalidad solar-econ√≥mica...")
        
        try:
            # Preparar datos
            economic_clean, solar_clean = self._prepare_correlation_data(economic_data, solar_data)
            
            # Crear DataFrame para VAR
            data = pd.DataFrame({
                'economic': economic_clean,
                'solar': solar_clean
            })
            
            # Test de Granger
            granger_results = self._granger_causality_test(data, max_lag)
            
            # Entrop√≠a de transferencia
            transfer_entropy = self._compute_transfer_entropy(economic_clean, solar_clean)
            
            # Mapeo cruzado convergente (simplificado)
            ccm = self._compute_convergent_cross_mapping(economic_clean, solar_clean)
            
            # Determinar direcci√≥n de causalidad
            direction, confidence = self._determine_causal_direction(granger_results, transfer_entropy)
            
            analysis = CausalAnalysis(
                cause='solar' if 'solar_to_economic' in direction else 'economic',
                effect='economic' if 'solar_to_economic' in direction else 'solar',
                granger_causality=granger_results,
                transfer_entropy=transfer_entropy,
                convergent_cross_mapping=ccm,
                confidence=confidence,
                direction=direction
            )
            
            logger.info(f"‚úÖ Causalidad analizada: {direction} (confianza: {confidence:.2f})")
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis de causalidad: {e}")
            raise
    
    def _granger_causality_test(self, data: pd.DataFrame, 
                              max_lag: int) -> Dict[str, float]:
        """Realizar test de causalidad de Granger"""
        results = {}
        
        try:
            # Test Granger: Solar -> Econ√≥mico
            test_result_solar_economic = grangercausalitytests(
                data[['economic', 'solar']], maxlag=max_lag, verbose=False
            )
            
            # Test Granger: Econ√≥mico -> Solar
            test_result_economic_solar = grangercausalitytests(
                data[['solar', 'economic']], maxlag=max_lag, verbose=False
            )
            
            # Extraer p-values para cada lag
            for lag in range(1, max_lag + 1):
                results[f'lag_{lag}_solar_to_economic'] = test_result_solar_economic[lag][0]['ssr_ftest'][1]
                results[f'lag_{lag}_economic_to_solar'] = test_result_economic_solar[lag][0]['ssr_ftest'][1]
                
        except Exception as e:
            logger.warning(f"Test de Granger fall√≥: {e}")
            # Valores por defecto si falla
            for lag in range(1, max_lag + 1):
                results[f'lag_{lag}_solar_to_economic'] = 1.0
                results[f'lag_{lag}_economic_to_solar'] = 1.0
        
        return results
    
    def _compute_transfer_entropy(self, x: np.ndarray, y: np.ndarray, 
                                k: int = 1) -> float:
        """Calcular entrop√≠a de transferencia (simplificada)"""
        # Implementaci√≥n b√°sica de transfer entropy
        n = len(x)
        if n < 10:
            return 0.0
        
        # Discretizar datos
        x_disc = self._discretize_data(x, bins=5)
        y_disc = self._discretize_data(y, bins=5)
        
        # Calcular entrop√≠as condicionales (implementaci√≥n simplificada)
        te_xy = mutual_info_score(x_disc[k:], y_disc[:-k]) if k < n else 0
        te_yx = mutual_info_score(y_disc[k:], x_disc[:-k]) if k < n else 0
        
        # Entrop√≠a de transferencia neta
        return te_xy - te_yx
    
    def _compute_convergent_cross_mapping(self, x: np.ndarray, y: np.ndarray) -> float:
        """Calcular mapeo cruzado convergente (simplificado)"""
        # Implementaci√≥n b√°sica de CCM
        n = len(x)
        if n < 20:
            return 0.0
        
        # Usar correlaci√≥n de retraso como proxy simple
        lags = range(1, min(13, n//2))
        ccm_scores = []
        
        for lag in lags:
            if lag < n:
                corr = np.corrcoef(x[lag:], y[:-lag])[0, 1]
                ccm_scores.append(abs(corr) if not np.isnan(corr) else 0)
        
        return np.mean(ccm_scores) if ccm_scores else 0.0
    
    def _determine_causal_direction(self, granger_results: Dict[str, float],
                                  transfer_entropy: float) -> Tuple[str, float]:
        """Determinar direcci√≥n de causalidad"""
        # Analizar resultados de Granger
        solar_to_economic_pvals = [v for k, v in granger_results.items() 
                                 if 'solar_to_economic' in k]
        economic_to_solar_pvals = [v for k, v in granger_results.items() 
                                 if 'economic_to_solar' in k]
        
        # Contar significancias (p < 0.05)
        sig_solar_economic = sum(1 for p in solar_to_economic_pvals if p < 0.05)
        sig_economic_solar = sum(1 for p in economic_to_solar_pvals if p < 0.05)
        
        # Determinar direcci√≥n basada en significancia y entrop√≠a
        if sig_solar_economic > sig_economic_solar and transfer_entropy > 0:
            direction = "solar_to_economic"
            confidence = sig_solar_economic / len(solar_to_economic_pvals)
        elif sig_economic_solar > sig_solar_economic and transfer_entropy < 0:
            direction = "economic_to_solar"
            confidence = sig_economic_solar / len(economic_to_solar_pvals)
        elif sig_solar_economic == sig_economic_solar and sig_solar_economic > 0:
            direction = "bidirectional"
            confidence = (sig_solar_economic + sig_economic_solar) / (
                len(solar_to_economic_pvals) + len(economic_to_solar_pvals))
        else:
            direction = "none"
            confidence = 0.0
        
        return direction, confidence
    
    def find_common_cycles(self) -> Dict[str, Any]:
        """Encontrar ciclos comunes entre dominios solar y econ√≥mico"""
        logger.info("üîÑ Buscando ciclos comunes solar-econ√≥micos...")
        
        common_cycles = {
            'high_confidence_cycles': [],
            'medium_confidence_cycles': [],
            'theoretical_cycles': [],
            'cycle_relationships': [],
            'synchronization_analysis': []
        }
        
        # Ciclos de alta confianza (basados en investigaci√≥n)
        common_cycles['high_confidence_cycles'].extend([
            {
                'name': 'Solar-Econ√≥mico ~11 a√±os',
                'period': 11.0,
                'strength': 0.75,
                'evidence': 'M√∫ltiples estudios correlaci√≥n manchas solares-mercados',
                'mechanism': 'Actividad solar ‚Üí Clima ‚Üí Agricultura ‚Üí Econom√≠a'
            }
        ])
        
        # Ciclos de media confianza
        common_cycles['medium_confidence_cycles'].extend([
            {
                'name': 'Kondratiev-Gleissberg',
                'period': 54.0,
                'strength': 0.65,
                'evidence': 'Sincronizaci√≥n te√≥rica ondas largas-ciclos solares extendidos',
                'mechanism': 'Ciclos tecnol√≥gicos influenciados por ambiente energ√©tico solar'
            },
            {
                'name': 'Kuznets-Solar',
                'period': 18.0,
                'strength': 0.55,
                'evidence': 'Correlaci√≥n infraestructura-actividad solar',
                'mechanism': 'Ciclos de construcci√≥n modulados por condiciones clim√°ticas solares'
            }
        ])
        
        # Relaciones entre ciclos
        common_cycles['cycle_relationships'].extend([
            {
                'relationship': '5 ciclos Schwabe ‚âà 1 ciclo Kondratiev',
                'ratio': 55/54,
                'deviation': 0.018,
                'significance': 'Alta',
                'implication': 'Posible sincronizaci√≥n a largo plazo'
            },
            {
                'relationship': '2 ciclos Kuznets ‚âà 1 ciclo Hale solar',
                'ratio': 36/22,
                'deviation': 0.636,
                'significance': 'Media',
                'implication': 'Sincronizaci√≥n generacional'
            }
        ])
        
        # An√°lisis de sincronizaci√≥n
        common_cycles['synchronization_analysis'] = self._analyze_cycle_synchronization()
        
        return common_cycles
    
    def _analyze_cycle_synchronization(self) -> List[CycleSynchronization]:
        """Analizar sincronizaci√≥n entre ciclos solares y econ√≥micos"""
        synchronizations = []
        
        # An√°lisis de ciclos conocidos
        cycle_pairs = [
            (11.0, 9.0, 'Schwabe-Juglar'),
            (22.0, 18.0, 'Hale-Kuznets'),
            (87.0, 54.0, 'Gleissberg-Kondratiev')
        ]
        
        for solar_period, economic_period, pair_name in cycle_pairs:
            period_ratio = economic_period / solar_period
            phase_diff = self._calculate_phase_difference(solar_period, economic_period)
            sync_strength = self._calculate_synchronization_strength(period_ratio, phase_diff)
            
            synchronization = CycleSynchronization(
                solar_cycle_period=solar_period,
                economic_cycle_period=economic_period,
                period_ratio=period_ratio,
                phase_difference=phase_diff,
                synchronization_strength=sync_strength,
                coherence=0.7,  # Valor te√≥rico
                historical_evidence=self._gather_historical_evidence(pair_name)
            )
            
            synchronizations.append(synchronization)
        
        return synchronizations
    
    def _calculate_phase_difference(self, solar_period: float, 
                                  economic_period: float) -> float:
        """Calcular diferencia de fase te√≥rica entre ciclos"""
        # Diferencia de fase normalizada (0-1)
        return abs(solar_period - economic_period) / max(solar_period, economic_period)
    
    def _calculate_synchronization_strength(self, period_ratio: float, 
                                          phase_diff: float) -> float:
        """Calcular fuerza de sincronizaci√≥n te√≥rica"""
        # Basado en qu√© tan cerca est√° la relaci√≥n de per√≠odo de n√∫meros racionales
        from fractions import Fraction
        
        try:
            # Encontrar fracci√≥n m√°s cercana
            target_ratio = period_ratio
            fraction = Fraction(target_ratio).limit_denominator(10)
            rational_approximation = float(fraction)
            
            # Calcular desviaci√≥n
            deviation = abs(target_ratio - rational_approximation) / target_ratio
            
            # Fuerza de sincronizaci√≥n (mayor para desviaciones menores)
            sync_strength = 1.0 - deviation - phase_diff
            
            return max(0.0, min(1.0, sync_strength))
            
        except:
            return 0.0
    
    def _gather_historical_evidence(self, cycle_pair: str) -> List[Dict[str, Any]]:
        """Recopilar evidencia hist√≥rica de sincronizaci√≥n"""
        evidence = {
            'Schwabe-Juglar': [
                {'event': 'Crisis 2008', 'solar_cycle': 23, 'economic_phase': 'Recesi√≥n'},
                {'event': 'Burbuja dot-com 2000', 'solar_cycle': 23, 'economic_phase': 'Sobreinversi√≥n'},
                {'event': 'Lunes Negro 1987', 'solar_cycle': 22, 'economic_phase': 'Correcci√≥n'}
            ],
            'Hale-Kuznets': [
                {'event': 'Crisis petr√≥leo 1973', 'solar_cycle': 20, 'economic_phase': 'Estanflaci√≥n'},
                {'event': 'Post-guerra boom', 'solar_cycle': 18, 'economic_phase': 'Expansi√≥n'}
            ],
            'Gleissberg-Kondratiev': [
                {'event': 'Gran Depresi√≥n', 'solar_cycle': 16, 'economic_phase': 'Invierno'},
                {'event': 'Revoluci√≥n Industrial', 'solar_cycle': 'M√≠nimo Dalton', 'economic_phase': 'Primavera'}
            ]
        }
        
        return evidence.get(cycle_pair, [])

# Instancia global para uso en otros m√≥dulos
correlation_service = CorrelationService()
